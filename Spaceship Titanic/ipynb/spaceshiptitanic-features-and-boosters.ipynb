{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"background-color:purple;\n           font-family:segoe ui;\n           color:black;\n           font-size:250%;\n           text-align:center;\n           border-radius:10px 10px;\">\n         ‚òÑÔ∏è Spaceship Titanic   üöÄ\n    \n<h1 style=\"background-color:black;\n           font-family:segoe ui;\n           color:white;\n           font-size:150%;\n           text-align:center;\">         \n          üåëWhich passengers are transported to an alternate dimensionüåë","metadata":{}},{"cell_type":"markdown","source":" <div>\n<div style=\"text-align: center;\"> <img src=\"https://github.com/OfirMazor/Kaggle/blob/main/Spaceship%20Titanic/img/DALLE2%20-%20Spaceship%20Titanic2.png?raw=true\" width=\"400\" alt=\"DALLE2: 'Spaceship Titanic'\"/>\n</div>","metadata":{}},{"cell_type":"markdown","source":"* \n<h1 style=\"text-align:Left; font-size:150%; color:#1c6ce6;\">\nLibraries üìö","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score\n\n\nimport xgboost as xgb\nimport lightgbm as lgbm","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.467401,"end_time":"2022-12-06T20:56:58.331820","exception":false,"start_time":"2022-12-06T20:56:56.864419","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:25.698246Z","iopub.execute_input":"2022-12-11T16:41:25.698642Z","iopub.status.idle":"2022-12-11T16:41:25.707165Z","shell.execute_reply.started":"2022-12-11T16:41:25.698612Z","shell.execute_reply":"2022-12-11T16:41:25.705636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* \n<h1 style=\"text-align:Left; font-size:150%; color:#1c6ce6;\">\nLoad data üóê","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntest_data  = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')","metadata":{"papermill":{"duration":0.08785,"end_time":"2022-12-06T20:56:58.423544","exception":false,"start_time":"2022-12-06T20:56:58.335694","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:25.723437Z","iopub.execute_input":"2022-12-11T16:41:25.724130Z","iopub.status.idle":"2022-12-11T16:41:25.779033Z","shell.execute_reply.started":"2022-12-11T16:41:25.724091Z","shell.execute_reply":"2022-12-11T16:41:25.778073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.sample(3)","metadata":{"papermill":{"duration":0.037386,"end_time":"2022-12-06T20:56:58.464914","exception":false,"start_time":"2022-12-06T20:56:58.427528","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:25.780889Z","iopub.execute_input":"2022-12-11T16:41:25.781487Z","iopub.status.idle":"2022-12-11T16:41:25.801368Z","shell.execute_reply.started":"2022-12-11T16:41:25.781451Z","shell.execute_reply":"2022-12-11T16:41:25.800289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* \n<h1 style=\"text-align:Left; font-size:150%; color:#1c6ce6;\">\nFeature engineering and data cleaning‚öôÔ∏è","metadata":{}},{"cell_type":"code","source":"def features_engine(df           : pd.DataFrame,\n                    train        : bool,\n                    Cabin_column : str,\n                    ID_Column    : str,\n                    Name_Column  : str,\n                    bools        : list,\n                    categoricals : list,\n                    money_cols   : list,\n                    age_col      : str,\n                    NaNs         : str):\n    '''\n    Full pipeline functions for processing a DataFrame.\n    Return the input DataFrame with features ready for model training.\n    '''\n    \n    num_rows = len(df)\n    #########################################################################################################################\n    \n    def Bool2int(df = df, bools = bools):\n        '''\n        Converts Boolean columns to integers (0,1).\n        '''\n        for column in bools:\n            try:\n                df[column] = df[column].astype('Int32')\n            except:\n                df[column] = df[column].astype(bool)\n                df[column] = df[column].astype('Int32')\n                \n        \n    #########################################################################################################################\n\n    def cabin_engine(df = df, Cabin_column = Cabin_column):\n        '''\n        Splits the Cabin description text to 3 different features,\n        filling Na's values where necessary.\n        '''\n        # Split\n        df[['CabinDeck','CabinNumber','CabinSide']] = df[Cabin_column].str.split('/', expand=True)\n        \n        # Type casting and missing values\n        df[['CabinDeck', 'CabinSide']] = df[['CabinDeck','CabinSide']].astype(str)\n        \n        df['CabinNumber'] = df['CabinNumber'].fillna('99999') #The new CabinNumber column created first as string\n        df['CabinNumber'] = df['CabinNumber'].astype(int)\n        df['CabinNumber'] = df['CabinNumber'].replace(99999, np.nan)\n        mf_imp = SimpleImputer(strategy=\"most_frequent\")\n        mf_imp.fit_transform(df[['CabinNumber']])\n        \n        \n        df.drop(columns = Cabin_column, inplace=True)\n\n    \n    #########################################################################################################################\n\n    def name_engine(df = df, Name_Column = Name_Column):\n        '''\n        Splits the full name text into 2 different features (FirstName, LastName).\n        Get the lenght of each first and last name as 2 new columns (FirstName_len, LastName_len).\n        Get the Family size (by assuming LastName is for each different family).\n        Get a boolean info if the passenger is part of a family or solo.\n        The original Name_Column will be dropped.\n        '''\n        \n        # Get first and last names\n        df[['FirstName','LastName']] = df[Name_Column].str.split(' ', expand=True)\n        df.drop(columns = Name_Column, inplace = True)\n        \n        # Calculate the names length\n        df['FirstName_len'] = df['FirstName'].str.len()\n        df['LastName_len']  = df['LastName'].str.len()\n        \n        \n        # Get family size before filling Na's (will be used next steps)\n        family_sizes = pd.DataFrame(df['LastName'].value_counts(dropna=True).sort_index())\n        family_sizes.rename(columns={'LastName' : 'FamilySize'}, inplace=True)\n        family_sizes['LastName'] = family_sizes.index\n        \n        # After missing names were count as 0 for their lenght-\n        # filling them with a temporary name \"No Name\".\n        df['FirstName'] = df['FirstName'].fillna('No')\n        df['LastName'] = df['LastName'].fillna('Name')\n        \n        # Back to those unnamed - replace their length column (=0) with mean length.\n        df['FirstName_len'].replace(0, df['FirstName_len'].mean(), inplace=True)\n        df['LastName_len'].replace(0, df['LastName_len'].mean(), inplace=True)\n        \n        \n        # Get Family size as FamilySize column \n        df['FamilySize'] = df.merge(family_sizes, on = 'LastName', how='left')['FamilySize']\n        df.loc[(df['FirstName'] == 'No')\n                       & \n               (df['LastName'] == 'Name'), 'FamilySize'] = 0\n        \n        # Get family validate as IsFamily column (0: solo passenger, 1: passenger part of family)\n        df['IsFamily'] = df['FamilySize'] > 1\n        df['IsFamily'] = df['IsFamily'].astype(int)\n        \n        \n    \n    #########################################################################################################################\n\n    def group_engine(df = df, ID_Column  = ID_Column):\n        '''\n        Adding 4 columns (IDGroup, NumberInGroup, GroupSize, 'IsGroup') from the input ID_Column\n        '''\n        # IDPrefix & NumberInGroup\n        df[['IDGroup', 'NumberInGroup']] = df[ID_Column].str.split('_', expand=True)\n        df[['IDGroup', 'NumberInGroup']] = df[['IDGroup', 'NumberInGroup']].astype(int)\n        \n        #GroupSize & IsGroup\n        group_sizes = pd.DataFrame(df['IDGroup'].value_counts(dropna=False).sort_index())\n        group_sizes.rename(columns={'IDGroup' : 'GroupSize'}, inplace=True)\n        group_sizes['IDGroup'] = group_sizes.index\n        \n        df['GroupSize'] = df.merge(group_sizes, on = 'IDGroup', how='left')['GroupSize']\n        \n        df['IsGroup'] = df['GroupSize'] > 1\n        df['IsGroup'] = df['IsGroup'].astype(int)\n    \n    \n    #########################################################################################################################\n\n    def age_engine(df = df, age_col = age_col):\n        '''\n        Adding a column ('AgeGroup') that cluster the ages into 4 different groups.\n        '''\n        df['Age'].fillna(df['Age'].mean(), inplace=True)\n        \n        criteria = [\n                    df[age_col].between(df[age_col].min(), 10),   #Children\n                    df[age_col].between(11,                20),   #Youth\n                    df[age_col].between(21,                40),   #Adults\n                    df[age_col].between(41,  df[age_col].max())   #Elders\n                   ]\n        \n        groups   = ['Children', 'Youth', 'Adults', 'Elders']\n        \n        df['AgeGroup'] = np.select(criteria, groups, np.nan)\n\n    #########################################################################################################################\n\n    def payment_engine(df = df, money_cols = money_cols):\n        '''\n        Adding a summerized column (TotalBill) for all expenses for each passenger,\n        Adding a mean column (AverageBill) for all expenses for each passenger,\n        Adding a boolean column (IsBill) if there is a bill,\n        adding a count column (CountBill) for all expenses for each passenger\n        '''\n        df['TotalBill'] = df[money_cols].sum(axis = 1)\n        df['AverageBill'] = df[money_cols].mean(axis = 1)\n        \n        df['IsBill']    = df['TotalBill'] > 0\n        df['IsBill']    = df['IsBill'].astype(int)\n        \n        df['CountBill'] = df[money_cols].replace(0, np.nan, inplace=False).count(axis=1, numeric_only=True)\n                            \n                        \n    #########################################################################################################################\n\n    def encode(df = df, categoricals = categoricals):\n        '''\n        Perform labels encoding for categorical columns.\n        '''\n        if 'AgeGroup' in df.columns:\n            categoricals.append('AgeGroup')\n        else:\n            pass\n        \n        for category in categoricals:\n            #df[category] = LabelEncoder().fit_transform(df[category])\n            df[category] = OrdinalEncoder().fit_transform(df[[category]])\n    \n    #########################################################################################################################\n\n    def Check_DuplicatedIDs(df, ID_column):\n        '''\n        Make sure there is no multiple information for the same passenger.\n        '''\n        duplicated = df[df.duplicated(ID_column, keep=False)].sort_values(ID_column)\n        len_duplicated = len(duplicated)\n        if len_duplicated == 0:\n            print(\"No duplicated ID's were found\")\n        else:\n            print(f\"There are {len_duplicated} duplicated ID's\")\n        \n    #########################################################################################################################\n\n    def NaNsProcess(df = df, train = train, NaNs = NaNs):\n        '''\n        Processing all NaNs values in DataFrame.\n        (Columns that already filled are ignored)\n        If NaNs is 'drop':\n        Drops all observations with NaN values if df if for training.\n        If NaNs is 'fill':\n        Fill Columns with sklearn Imputer.\n        '''\n        if NaNs == 'drop':\n            if train:\n                print(f'DataFrame obsevations before NAs dropping:{len(df)}.')\n                df.dropna(inplace=True)\n                print(f'DataFrame obsevations after NAs dropping:{len(df)}.')\n            else:\n                pass\n        \n        elif NaNs == 'fill':\n            \n            ###  CryoSleep   ###\n            #Fill Na values as cryo sleepres (True) for passenger who didn't spent any money.\n            df.loc[\n                   (df['RoomService']  == 0)\n                                & \n                   (df['FoodCourt']    == 0)\n                                &\n                   (df['ShoppingMall'] == 0)\n                                &\n                   (df['Spa']          == 0)\n                                &\n                   (df['VRDeck']       == 0)\n                                &\n                   (df['CryoSleep'].isna()), 'CryoSleep'] = 1 # True\n            \n            df['CryoSleep'] = df['CryoSleep'].fillna(0)       #False\n            \n            \n            ###  Fill Na's with most frequent values   ###\n            \n            mf_imp   = SimpleImputer(strategy=\"most_frequent\")\n            \n            most_freq_cols = ['HomePlanet', 'Destination', 'VIP', 'CabinNumber', \n                              'CabinDeck', 'CabinSide', 'AgeGroup', 'IsBill']\n            \n            for column in most_freq_cols:\n                mf_imp.fit(df[[column]])\n                df[column] = mf_imp.transform(df[[column]])\n                \n                \n            ###  Fill Na's with mean values   ###\n            mean_imp = SimpleImputer(strategy=\"mean\")\n                \n            mean_cols      = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', \n                              'FirstName_len', 'LastName_len', 'TotalBill', 'CountBill', 'AverageBill']\n            \n            for column in mean_cols:\n                mean_imp.fit(df[[column]])\n                df[column] = mean_imp.transform(df[[column]])\n                \n            \n            \n            is_na = df.isna().sum().sum() > 0\n            if is_na:\n                print(\"WARNING: DataFrame still has Na's values\")\n            else:\n                pass\n            \n        else:\n            print(\"NaNs parameter should be one of ('drop', 'fill')\")\n        \n    #########################################################################################################################\n    '''\n    Execute the pipeline workflow by order\n    '''\n    Bool2int(df, bools)\n    cabin_engine(df, Cabin_column)\n    name_engine(df, Name_Column)\n    group_engine(df, ID_Column)\n    age_engine(df, age_col)\n    payment_engine(df, money_cols)\n    NaNsProcess(df)\n    encode(df, categoricals)\n    Check_DuplicatedIDs(df, ID_Column)\n    \n    \n    lost = num_rows - len(df)\n    if lost != 0: print(f'Warning : {lost} observations were dropped')\n    else: \n        pass\n    \n              \n    return  df","metadata":{"papermill":{"duration":0.035926,"end_time":"2022-12-06T20:56:58.505023","exception":false,"start_time":"2022-12-06T20:56:58.469097","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:25.951836Z","iopub.execute_input":"2022-12-11T16:41:25.952585Z","iopub.status.idle":"2022-12-11T16:41:25.995378Z","shell.execute_reply.started":"2022-12-11T16:41:25.952548Z","shell.execute_reply":"2022-12-11T16:41:25.994097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_train_data = features_engine(df           = train_data,\n                                     train        = True,\n                                     Cabin_column = 'Cabin',\n                                     ID_Column    = 'PassengerId',\n                                     Name_Column  = 'Name',\n                                     bools        = ['Transported', 'CryoSleep', 'VIP'],\n                                     categoricals = ['HomePlanet', 'Destination', 'CabinDeck', 'CabinSide'],\n                                     money_cols   = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'],\n                                     age_col      = 'Age',\n                                     NaNs         = 'fill')\n\n\n\n\nprocess_train_data = features_engine(df           = test_data,\n                                     train        = False,\n                                     Cabin_column = 'Cabin',\n                                     ID_Column    = 'PassengerId',\n                                     Name_Column  = 'Name',\n                                     bools        = ['CryoSleep', 'VIP'],\n                                     categoricals = ['HomePlanet', 'Destination', 'CabinDeck', 'CabinSide'],\n                                     money_cols   = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'],\n                                     age_col      = 'Age',\n                                     NaNs         = 'fill')","metadata":{"papermill":{"duration":0.407614,"end_time":"2022-12-06T20:56:58.917059","exception":false,"start_time":"2022-12-06T20:56:58.509445","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:25.998476Z","iopub.execute_input":"2022-12-11T16:41:25.999049Z","iopub.status.idle":"2022-12-11T16:41:26.487879Z","shell.execute_reply.started":"2022-12-11T16:41:25.999009Z","shell.execute_reply":"2022-12-11T16:41:26.486672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.sample(4)","metadata":{"papermill":{"duration":0.038504,"end_time":"2022-12-06T20:56:58.959901","exception":false,"start_time":"2022-12-06T20:56:58.921397","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:26.489144Z","iopub.execute_input":"2022-12-11T16:41:26.489448Z","iopub.status.idle":"2022-12-11T16:41:26.527928Z","shell.execute_reply.started":"2022-12-11T16:41:26.489420Z","shell.execute_reply":"2022-12-11T16:41:26.526582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* \n<h1 style=\"text-align:Left; font-size:150%; color:#1c6ce6;\">\nData Visualisations üìä","metadata":{}},{"cell_type":"code","source":"corr = abs(train_data.corr()['Transported']).sort_values().drop('Transported')\npx.bar(corr, title='Absolute correlations of all features to Transported column')","metadata":{"papermill":{"duration":0.39581,"end_time":"2022-12-06T20:56:59.360140","exception":false,"start_time":"2022-12-06T20:56:58.964330","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:26.530424Z","iopub.execute_input":"2022-12-11T16:41:26.530786Z","iopub.status.idle":"2022-12-11T16:41:26.614306Z","shell.execute_reply.started":"2022-12-11T16:41:26.530754Z","shell.execute_reply":"2022-12-11T16:41:26.613105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sample2Plot = train_data.sample(500)\n#sns.pairplot(data = Sample2Plot, hue = 'Transported', corner = True);","metadata":{"papermill":{"duration":0.015434,"end_time":"2022-12-06T20:56:59.380420","exception":false,"start_time":"2022-12-06T20:56:59.364986","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:26.615647Z","iopub.execute_input":"2022-12-11T16:41:26.615995Z","iopub.status.idle":"2022-12-11T16:41:26.622979Z","shell.execute_reply.started":"2022-12-11T16:41:26.615964Z","shell.execute_reply":"2022-12-11T16:41:26.621505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* \n<h1 style=\"text-align:Left; font-size:150%; color:#1c6ce6;\">\nModel and TrainingüèãÔ∏è","metadata":{}},{"cell_type":"code","source":"def TrainClassifier(train_df   : pd.DataFrame,\n                    test_df    : pd.DataFrame,\n                    scale      : bool,\n                    test_size  : int or float,\n                    features   : list,\n                    target     : str,\n                    classifier : str,\n                    clf_params : dict,\n                    seed       : int):\n\n    '''\n    Perform full training and testing steps of XBG classifier, ploting the metrics and returning submission file ready to submit.\n    Parameters:\n        - train_df   : DataFrame to train,\n        - test_df    : DataFrame to validate model,\n        - scale      : True if scale is wanted,\n        - test_size  : The size of train_df to consider as test,\n        - features   : List of desired feature to train the model,\n        - target     : The name of the target column in train_df/test_df,\n        - classifier : The name of the classifier model to use- XGB or LGBM,\n        - clf_params : Dictionary containing the classifier parameters,\n        - seed       : Random number seed\n    '''\n                                           #### Training Step ####\n    \n    # 0. Scale data \n    if scale:\n        #Scaler = StandardScaler()\n        Scaler = RobustScaler()\n        #Scaler = MinMaxScaler()\n        train_df[features] = Scaler.fit_transform(train_df[features])\n        test_df[features]  = Scaler.transform(test_df[features])\n    \n    else:\n        pass\n    \n    # 1. Split train_df  to trainig-testing\n    X_train, X_test, y_train, y_test = train_test_split(train_df[features],\n                                                        train_df[target].astype(int),\n                                                        test_size    = test_size,\n                                                        random_state = seed,\n                                                        stratify     = train_df[target])\n\n    \n    \n    # 2. Train a classifier and predict\n    if classifier == 'XGB':\n        model = xgb.XGBClassifier()\n        \n    elif classifier == 'LGBM':\n        model = lgbm.LGBMClassifier(objective    = 'binary',\n                                    random_state = seed,\n                                    silent  = True,\n                                    verbose = -1)\n    else:\n        print(\"classifier parameter should be 'XGB' or 'LGBM' \")\n    \n    \n    RSCV = RandomizedSearchCV(estimator = model,\n                              param_distributions = clf_params,\n                              cv      = 7,\n                              n_iter  = 20, \n                              scoring = 'accuracy')\n    \n    RSCV.fit(X_train, y_train)\n    \n    best_params = RSCV.best_params_\n    \n    if classifier == 'XGB':\n        best_model = xgb.XGBClassifier(**best_params)\n    elif classifier == 'LGBM':\n        best_model = lgbm.LGBMClassifier(**best_params)\n    best_model.fit(X_train, y_train)\n    \n    y_pred = best_model.predict(X_test)\n    \n    print('F1 score:', f1_score(list(y_test.values), y_pred))\n    print('\\n')\n    print('Accuracy score:', accuracy_score(list(y_test.values), y_pred))\n    \n    #########################################################################################################################\n    \n    \n                                      #### Testing Step ####\n    \n    # 1. Predict validation data\n    test_df[target] = best_model.predict(test_df[features]).astype(bool)\n    \n    # 2. Submission format variable\n    submission = test_df[['PassengerId', target]]\n    \n    #########################################################################################################################\n\n    \n                                      #### Review and Metrics Step ####\n    \n    # 1.Review classifier parameters\n    try:\n        display(pd.DataFrame(best_model.get_xgb_params(), index=[0]))\n    except:\n        display(pd.DataFrame(best_model.get_params(), index=[0]))\n        \n    \n    \n    # 2.Review classifier features by importance\n    features_importance = pd.DataFrame(data = best_model.feature_importances_).T\n    features_importance.columns = features\n    px.bar(features_importance.T, title = 'Features Importance for Best Model').show()\n    \n    \n    # 3.Plot confusion metrix\n    cm = confusion_matrix(y_true    = list(y_test.values),\n                          y_pred    = y_pred,\n                          normalize = 'true')\n    \n    cm_Display = ConfusionMatrixDisplay(confusion_matrix = cm,\n                                        display_labels   = [False, True])\n    cm_Display.plot()\n    cm_Display.ax_.set_title(\"Confusion Matrix\");\n    \n    # 4. Plot value counts of prediction\n    px.bar(submission[target].value_counts(dropna=False),\n           title = f'Value Counts of {target} in Submission').show()\n    \n    \n    return submission","metadata":{"papermill":{"duration":0.020536,"end_time":"2022-12-06T20:56:59.405702","exception":false,"start_time":"2022-12-06T20:56:59.385166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:26.625242Z","iopub.execute_input":"2022-12-11T16:41:26.625709Z","iopub.status.idle":"2022-12-11T16:41:26.646968Z","shell.execute_reply.started":"2022-12-11T16:41:26.625665Z","shell.execute_reply":"2022-12-11T16:41:26.645831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\n            #'Age',\n            'AgeGroup',\n            'AverageBill',\n            'CabinDeck',\n            'CabinNumber',\n            'CabinSide',\n            'CountBill',\n            'CryoSleep',\n            'Destination',\n            'FamilySize',\n            #'FirstName',\n            'FirstName_len',\n            'FoodCourt',\n            'GroupSize',\n            'HomePlanet',\n            'IDGroup',\n            'IsBill',\n            'IsFamily',\n            'IsGroup',\n            #'LastName',\n            'LastName_len',\n            'NumberInGroup',\n            #'PassengerId',\n            'RoomService',\n            'ShoppingMall',\n            'Spa',\n            'TotalBill',\n            #'VIP',\n            'VRDeck'\n           ]\n\n\n\nxgbClassifier_params = {'booster'         : ['gbtree','gblinear'],\n                        'learning_rate'   : [0.2, 0.1, 0.05], \n                        'max_depth'       : [3, 4, 5, 10],\n                        'min_child_weight': [10, 15, 20],\n                        'n_estimators'    : [50, 300, 600],\n                        'eval_metric'     : ['auc', 'mlogloss']}            \n#https://xgboost.readthedocs.io/en/stable/parameter.html\n\n\nlgbmClassifier_params = {'boosting_type' : ['gbdt', 'dart'],\n                         'num_leaves'    : [10, 15, 20], \n                         'max_depth'     : [3, 4, 5, 10],\n                         'learning_rate' : [0.2, 0.1, 0.05],\n                         'n_estimators'  : [50, 300, 600],\n                         'metric'        : ['auc','binary_logloss']}             \n#https://lightgbm.readthedocs.io/en/v3.3.2/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier\n\n\n\nsubmission = TrainClassifier(train_df  = train_data,\n                            test_df    = test_data,\n                            scale      = True,\n                            test_size  = 0.2,\n                            features   = features,\n                            target     = 'Transported',\n                            classifier = 'LGBM',                 # 'XGB'\n                            clf_params = lgbmClassifier_params,  # xgbClassifier_params\n                            seed       = 100)","metadata":{"papermill":{"duration":1.001203,"end_time":"2022-12-06T20:57:00.411808","exception":false,"start_time":"2022-12-06T20:56:59.410605","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:41:26.648929Z","iopub.execute_input":"2022-12-11T16:41:26.650053Z","iopub.status.idle":"2022-12-11T16:43:43.579701Z","shell.execute_reply.started":"2022-12-11T16:41:26.650006Z","shell.execute_reply":"2022-12-11T16:43:43.578856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* \n<h1 style=\"text-align:Left; font-size:150%; color:#1c6ce6;\">\nSubmit Results üéØ","metadata":{}},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.022228,"end_time":"2022-12-06T20:57:00.439425","exception":false,"start_time":"2022-12-06T20:57:00.417197","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-11T16:43:43.581940Z","iopub.execute_input":"2022-12-11T16:43:43.582426Z","iopub.status.idle":"2022-12-11T16:43:43.595019Z","shell.execute_reply.started":"2022-12-11T16:43:43.582380Z","shell.execute_reply":"2022-12-11T16:43:43.593696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div>\n<div style=\"text-align: center;\"> <img src=\"https://github.com/OfirMazor/Kaggle/blob/main/Spaceship%20Titanic/img/DALLE2%20-%20Spaceship%20Titanic1.png?raw=true\" width=\"400\" alt=\"DALLE2: 'Spaceship Titanic'\"/>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Spaceships images generated with DALLE2","metadata":{}}]}